import streamlit as st  # Import the Streamlit library for creating web applications
import os  # Import the os module for interacting with the operating system
from langchain_community.document_loaders import PyPDFLoader  # Import PyPDFLoader to load PDF documents

from langchain_text_splitters import RecursiveCharacterTextSplitter  # Import RecursiveCharacterTextSplitter for splitting text recursively
from langchain_google_vertexai import VertexAIEmbeddings  # Import VertexAIEmbeddings for creating embeddings using Google Vertex AI
from langchain_core.vectorstores import InMemoryVectorStore  # Import InMemoryVectorStore to store vectors in memory
from langchain_openai import OpenAIEmbeddings  # Import OpenAIEmbeddings for creating embeddings using OpenAI (commented out)
from google.oauth2 import service_account  # Import service_account for handling Google service account credentials
# from langchain.chat_models import init_chat_model # Deprecated, use specific chat model classes
from langchain_core.prompts import PromptTemplate  # Import PromptTemplate for creating prompt templates
from langchain.retrievers import ParentDocumentRetriever  # Import ParentDocumentRetriever for retrieving documents based on parent-child relationships
from langchain.storage import InMemoryStore  # Import InMemoryStore for storing documents in memory

# from langchain_experimental.text_splitter import SemanticChunker # Import SemanticChunker (commented out)

# Load Google service account credentials from a JSON file
egat_service_account = service_account.Credentials.from_service_account_file(
    filename="./service_account.json"
)

file_path = "./Regulation_389.pdf"  # Define the path to the PDF document
os.environ["OPENAI_API_KEY"] = st.secrets.llm.openai_api_key  # Set the OpenAI API key from Streamlit secrets (even though OpenAIEmbeddings is commented out, this is likely for an alternative LLM)

# Initialize Vertex AI embeddings
embeddings = VertexAIEmbeddings(
    model="text-embedding-005", credentials=egat_service_account
)
# embeddings = OpenAIEmbeddings(model="text-embedding-3-large") # Initialize OpenAI embeddings (commented out)

# Initialize in-memory vector store and document store
vector_store = InMemoryVectorStore(embeddings)
store = InMemoryStore()

# Initialize text splitters for parent and child documents
parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)
child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)

# Initialize the ParentDocumentRetriever
retriever = ParentDocumentRetriever(
    vectorstore=vector_store,  # The vector store for storing child document embeddings
    docstore=store,  # The document store for storing parent documents
    parent_splitter=parent_splitter,  # The splitter for creating parent documents
    child_splitter=child_splitter,  # The splitter for creating child documents
)


def load_data():
    """
    Loads the PDF document, splits it into parent and child documents,
    and adds them to the retriever's vector and document stores.
    """
    st.write("Loading data...")  # Display a message indicating data loading
    loader = PyPDFLoader(file_path)  # Create a PDF loader instance
    docs = loader.load()  # Load the documents from the PDF

    # Function to replace Thai numerals with Arabic numerals (commented out)
    # def replace_thai_no(text):
    #       return text.replace("‡πê", "0").replace("‡πë", "1").replace("‡πí", "2").replace("‡πì", "3").replace("‡πî", "4").replace("‡πï", "5").replace("‡πñ", "6").replace("‡πó", "7").replace("‡πò", "8").replace("‡πô", "9")

    st.write("Document Page", len(docs))  # Display the number of loaded document pages
    retriever.add_documents(docs)  # Add the loaded documents to the retriever

    # Perform a similarity search on the vector store with a sample query
    sub_docs = vector_store.similarity_search("‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏ã‡∏∑‡πâ‡∏≠‡∏°‡∏µ‡∏Å‡∏µ‡πà‡∏ß‡∏¥‡∏ò‡∏µ", k=3)
    st.write("Sub Documents", sub_docs)  # Display the results of the similarity search

    # Retrieve documents using the parent document retriever with a sample query
    retrievered_docs = retriever.invoke("‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏ã‡∏∑‡πâ‡∏≠‡∏°‡∏µ‡∏Å‡∏µ‡πà‡∏ß‡∏¥‡∏ò‡∏µ")
    st.write("Retrieved Documents", retrievered_docs)  # Display the retrieved documents


# Initialize the chat model using Google Vertex AI
from langchain_google_vertexai import ChatVertexAI
llm = ChatVertexAI(
    model="gemini-2.0-flash-001",
    credentials=egat_service_account,
)
# llm = init_chat_model("gpt-4o", model_provider="openai") # Initialize OpenAI chat model (commented out)

# Define the prompt template for the language model
template = """
‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏ã‡∏∑‡πâ‡∏≠‡∏à‡∏±‡∏î‡∏à‡πâ‡∏≤‡∏á‡∏Ç‡∏≠‡∏á ‡∏Å‡∏ü‡∏ú. ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÅ‡∏ô‡∏ö‡∏à‡∏≤‡∏Å Retrieved Documents ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡πÑ‡∏ß‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏´‡∏•‡∏±‡∏Å‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡πÅ‡∏´‡∏•‡πà‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß

üìå ‡∏Å‡∏é‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö:
1. ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡πÇ‡∏î‡∏¢‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡πÑ‡∏ß‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
2. ‡∏´‡πâ‡∏≤‡∏°‡πÅ‡∏ï‡πà‡∏á‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ô‡∏≠‡∏Å‡πÄ‡∏´‡∏ô‡∏∑‡∏≠‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
3. ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤ ‚Äú‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡πÑ‡∏ß‡πâ‚Äù
4. ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏ï‡∏≠‡∏ö ‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏∏‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á ‡πÄ‡∏ä‡πà‡∏ô ‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤, ‡∏´‡∏°‡∏ß‡∏î, ‡∏Ç‡πâ‡∏≠ ‡∏´‡∏£‡∏∑‡∏≠‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠
5. **‡∏ï‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏à‡πâ‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏ó‡∏µ‡πà‡∏£‡∏π‡πâ‡∏à‡∏£‡∏¥‡∏á‡∏ï‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏±‡∏ô‡πÄ‡∏≠‡∏á ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ**
6. **‡∏´‡πâ‡∏≤‡∏°‡∏ï‡∏≠‡∏ö‡πÄ‡∏ß‡∏¥‡πà‡∏ô‡πÄ‡∏ß‡πâ‡∏≠ ‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏Å‡∏¥‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£**
7. **‡∏ï‡∏≠‡∏ö‡∏ï‡∏£‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô ‡πÅ‡∏•‡∏∞‡∏™‡∏±‡πâ‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢**

üìå ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö:
- ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏™‡∏±‡πâ‡∏ô ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡∏ï‡∏£‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô
- ‡∏ï‡πà‡∏≠‡∏ó‡πâ‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÉ‡∏´‡∏°‡πà‡∏ß‡πà‡∏≤:

  _‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å: ‡∏´‡∏ô‡πâ‡∏≤ XX, ‡∏´‡∏°‡∏ß‡∏î X, ‡∏Ç‡πâ‡∏≠ X_

üìå ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö:
‚Äú‡∏ß‡∏á‡πÄ‡∏á‡∏¥‡∏ô‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 100,000 ‡∏ö‡∏≤‡∏ó ‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á‡πÑ‡∏î‡πâ ‡πÅ‡∏ï‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡πÅ‡∏•‡∏∞‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡∏°‡∏µ‡∏≠‡∏≥‡∏ô‡∏≤‡∏à‚Äù

_‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å: ‡∏´‡∏ô‡πâ‡∏≤ 12, ‡∏´‡∏°‡∏ß‡∏î 2, ‡∏Ç‡πâ‡∏≠ 5.1_

‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏≠‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô


## Retrieved Documents !!!!:
{retrieved_documents}

## ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô !!!!:
{user_prompt}
"""


def llm_response(messages):
    """
    Generates a response from the language model based on the user's prompt
    and retrieved documents.

    Args:
        messages (list): A list of message dictionaries, typically from a chat history.

    Returns:
        str: The content of the language model's response.
    """
    user_prompt = messages[-1]["content"]  # Get the latest user prompt from the messages
    print("user_prompt", user_prompt)  # Print the user prompt for debugging

    # retriever = vector_store.as_retriever(search_kwargs={"k": 10}) # Convert vector store to a retriever (commented out)
    retrieved_docs = retriever.invoke(user_prompt)  # Retrieve relevant documents based on the user prompt

    with st.expander("Retrieved Documents"):
        st.write(retrieved_docs)  # Display the retrieved documents in an expandable section

    custom_template = PromptTemplate.from_template(template)  # Create a PromptTemplate from the defined template

    def replace_thai_no(text):
        """Replaces Thai numerals with Arabic numerals in a given text."""
        return (
            text.replace("‡πê", "0")
            .replace("‡πë", "1")
            .replace("‡πí", "2")
            .replace("‡πì", "3")
            .replace("‡πî", "4")
            .replace("‡πï", "5")
            .replace("‡πñ", "6")
            .replace("‡πó", "7")
            .replace("‡πò", "8")
            .replace("‡πô", "9")
        )

    # Concatenate the content of the retrieved documents, replacing Thai numerals
    docs_content = "\n\n".join(
        replace_thai_no(doc.page_content) for doc in retrieved_docs
    )
    # st.write(docs_content) # Display the concatenated document content (commented out)

    # Invoke the prompt template with the user prompt and retrieved document content
    prompt = custom_template.invoke(
        {"user_prompt": user_prompt, "retrieved_documents": docs_content}
    )

    print("Final Prompt", prompt)  # Print the final prompt sent to the LLM for debugging

    response = llm.invoke(prompt)  # Invoke the language model with the final prompt
    return response.content  # Return the content of the language model's response
