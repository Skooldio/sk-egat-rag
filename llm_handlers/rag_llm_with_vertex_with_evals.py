‚Äú# Import necessary libraries
import streamlit as st # For potential Streamlit integration (though not directly used in this evaluation script)
import os # For interacting with the operating system (e.g., environment variables)
from langchain_community.document_loaders import PyPDFLoader # PDF Loader (not used here)
from langchain_text_splitters import RecursiveCharacterTextSplitter # Text Splitter (not used here)
from langchain_google_vertexai import VertexAIEmbeddings # Vertex AI Embeddings (not used directly here, handled by Vertex AI Search)
from langchain_core.vectorstores import InMemoryVectorStore # In-memory store (not used here)
from langchain_openai import OpenAIEmbeddings # OpenAI Embeddings (not used here)
from google.oauth2 import service_account # For Google Cloud credentials
from langchain.chat_models import init_chat_model # Function to initialize chat models
from langchain_core.prompts import PromptTemplate # For creating prompt templates
from langchain_google_community import (
    VertexAISearchRetriever, # Retriever for Google Cloud Vertex AI Search
)
from langchain.schema import StrOutputParser # Simple output parser for LangChain chains
from langchain_core.runnables import RunnablePassthrough # Passthrough component for LangChain Expression Language (LCEL)

# Load Google Cloud service account credentials from a local JSON file.
egat_service_account = service_account.Credentials.from_service_account_file(
    filename="./service_account.json"
)

# Initialize the Google Gemini LLM via Vertex AI using the loaded credentials.
llm = init_chat_model(
    "gemini-2.0-flash-001", # Specify the Gemini model version
    model_provider="google_vertexai", # Indicate the provider
    credentials=egat_service_account, # Pass the credentials
)
# llm = init_chat_model("gpt-4o", model_provider="openai") # Alternative: Initialize OpenAI model (commented out)

# Define the prompt template string for the LLM.
# This template guides the LLM to answer questions based on retrieved EGAT documents.
template = """
‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏ã‡∏∑‡πâ‡∏≠‡∏à‡∏±‡∏î‡∏à‡πâ‡∏≤‡∏á‡∏Ç‡∏≠‡∏á ‡∏Å‡∏ü‡∏ú. ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÅ‡∏ô‡∏ö‡∏à‡∏≤‡∏Å Retrieved Documents ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡πÑ‡∏ß‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏´‡∏•‡∏±‡∏Å‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡πÅ‡∏´‡∏•‡πà‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß

üìå ‡∏Å‡∏é‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö:
1. ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡πÇ‡∏î‡∏¢‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡πÑ‡∏ß‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
2. ‡∏´‡πâ‡∏≤‡∏°‡πÅ‡∏ï‡πà‡∏á‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ô‡∏≠‡∏Å‡πÄ‡∏´‡∏ô‡∏∑‡∏≠‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
3. ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤ ‚Äú‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡πÑ‡∏ß‡πâ‚Äù
4. ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏ï‡∏≠‡∏ö ‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏∏‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á ‡πÄ‡∏ä‡πà‡∏ô ‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤, ‡∏´‡∏°‡∏ß‡∏î, ‡∏Ç‡πâ‡∏≠ ‡∏´‡∏£‡∏∑‡∏≠‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠
5. **‡∏ï‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏à‡πâ‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏ó‡∏µ‡πà‡∏£‡∏π‡πâ‡∏à‡∏£‡∏¥‡∏á‡∏ï‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏±‡∏ô‡πÄ‡∏≠‡∏á ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ**
6. **‡∏´‡πâ‡∏≤‡∏°‡∏ï‡∏≠‡∏ö‡πÄ‡∏ß‡∏¥‡πà‡∏ô‡πÄ‡∏ß‡πâ‡∏≠ ‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏Å‡∏¥‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£**
7. **‡∏ï‡∏≠‡∏ö‡∏ï‡∏£‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô ‡πÅ‡∏•‡∏∞‡∏™‡∏±‡πâ‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢**

üìå ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö:
- ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏™‡∏±‡πâ‡∏ô ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡∏ï‡∏£‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô
- ‡∏ï‡πà‡∏≠‡∏ó‡πâ‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÉ‡∏´‡∏°‡πà‡∏ß‡πà‡∏≤:

  _‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å: ‡∏´‡∏ô‡πâ‡∏≤ XX, ‡∏´‡∏°‡∏ß‡∏î X, ‡∏Ç‡πâ‡∏≠ X_

üìå ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö:
‚Äú‡∏ß‡∏á‡πÄ‡∏á‡∏¥‡∏ô‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 100,000 ‡∏ö‡∏≤‡∏ó ‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á‡πÑ‡∏î‡πâ ‡πÅ‡∏ï‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡πÅ‡∏•‡∏∞‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡∏°‡∏µ‡∏≠‡∏≥‡∏ô‡∏≤‡∏à‚Äù

_‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å: ‡∏´‡∏ô‡πâ‡∏≤ 12, ‡∏´‡∏°‡∏ß‡∏î 2, ‡∏Ç‡πâ‡∏≠ 5.1_

‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏≠‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô


## Retrieved Documents !!!!:
{retrieved_documents}

## ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô !!!!:
{user_prompt}
"""

# --- Vertex AI Search Configuration ---
PROJECT_ID = ""  # Your Google Cloud Project ID
LOCATION_ID = "global"  # Location of your Vertex AI Search data store
SEARCH_ENGINE_ID = ""  # Your Vertex AI Search App ID (Engine ID)
DATA_STORE_ID = "" # Your Vertex AI Search Data Store ID
# --- End Vertex AI Search Configuration ---

# Helper function to format the retrieved documents into a single string.
def format_docs(docs):
    """Concatenates the page_content of retrieved LangChain Documents."""
    return "\n\n".join(doc.page_content for doc in docs)

# Create a PromptTemplate object from the template string.
custom_template = PromptTemplate.from_template(template)

# Initialize the Vertex AI Search Retriever.
retriever = VertexAISearchRetriever(
    credentials=egat_service_account, # Pass credentials
    project_id=PROJECT_ID, # Specify Project ID
    location_id=LOCATION_ID, # Specify Data Store Location
    data_store_id=DATA_STORE_ID, # Specify Data Store ID
    max_documents=10, # Maximum number of documents to retrieve
    engine_data_type=0,  # Corresponds to 'unstructured' data store type
    get_extractive_answers=True, # Attempt to get direct answers from the search results
)

# Define the RAG chain using LangChain Expression Language (LCEL).
rag_chain = (
    {
        # The 'retrieved_documents' key will hold the formatted content from the retriever.
        "retrieved_documents": retriever | format_docs, # Pipe output of retriever to format_docs
        # The 'user_prompt' key passes the original input through.
        "user_prompt": RunnablePassthrough(), # Pass the input query directly
    }
    | custom_template # Pipe the dictionary to the prompt template
    | llm # Pipe the formatted prompt to the LLM
    | StrOutputParser() # Parse the LLM output to a string
)

# (Commented out) Original function for getting LLM response, now replaced by direct chain invocation for evaluation.
# def llm_response(messages):
#     user_prompt = messages[-1]["content"]
#     return rag_chain.invoke(user_prompt)

# --- Trulens Evaluation Setup ---
from trulens.core.session import TruSession # Import TruSession for managing evaluation sessions

# Initialize or get the default Trulens session.
session = TruSession()
# Reset the database (clears previous evaluation records for this session).
session.reset_database()

from trulens.dashboard import run_dashboard # Import function to run the Trulens dashboard

# Start the Trulens dashboard (runs as a separate process/web server).
# 'force=True' restarts the dashboard if it's already running.
run_dashboard(session=session, force=True)

# Set OpenAI API key - Required for the Trulens OpenAI provider used for evaluations.
# WARNING: Hardcoding API keys is insecure. Use secrets management in production.
os.environ["OPENAI_API_KEY"] = ""

from trulens.providers.openai import OpenAI # Import the Trulens OpenAI provider

# Initialize the OpenAI provider (used for evaluation metrics, not the main RAG LLM).
provider = OpenAI()

from trulens.apps.langchain import TruChain # Import TruChain for instrumenting LangChain apps

# Select the part of the RAG chain that represents the retrieved context.
# This allows Trulens to evaluate the relevance of the documents retrieved by 'retriever'.
context = TruChain.select_context(rag_chain)

from trulens.core import Feedback # Import the Feedback class for defining evaluation metrics
import numpy as np # Import numpy for aggregation functions (like mean)

# Define a feedback function for Context Relevance.
# Uses the OpenAI provider to evaluate if the retrieved context is relevant to the input query.
f_context_relevance = (
    Feedback(provider.context_relevance_with_cot_reasons, name="Context Relevance") # Use OpenAI's CoT method
    .on_input() # Operates on the main input query
    .on(context) # Evaluates the selected context
    .aggregate(np.mean) # Averages the scores if multiple contexts are evaluated
)

# Define a feedback function for Groundedness.
# Uses the OpenAI provider to check if the LLM's response is supported by the retrieved context.
f_groundedness = (
    Feedback(provider.groundedness_measure_with_cot_reasons, name="Groundedness") # Use OpenAI's CoT method
    .on(context.collect())  # Collect context chunks into a list before evaluation
    .on_output() # Evaluates the final output response
)

# Define a feedback function for Answer Relevance.
# Uses the OpenAI provider to evaluate if the LLM's response is relevant to the input query.
f_answer_relevance = Feedback(
    provider.relevance_with_cot_reasons, name="Answer Relevance" # Use OpenAI's CoT method
).on_input_output() # Operates on both the input query and the final output response
# --- End Trulens Evaluation Setup ---

# Instrument the LangChain RAG chain with Trulens for evaluation.
tru_recorder = TruChain(
    rag_chain, # The LangChain chain to evaluate
    app_name="ChatApplication", # Name for the application in Trulens dashboard
    app_version="Chain1", # Version identifier
    feedbacks=[f_context_relevance, f_groundedness, f_answer_relevance], # List of feedback functions to apply
)

# Run example queries through the instrumented chain.
# The 'with tru_recorder as recording:' block ensures that each invocation is recorded by Trulens.
with tru_recorder as recording:
    # Invoke the chain with several test prompts.
    # Each invocation will be evaluated using the defined feedback functions.
    # Results can be viewed in the Trulens dashboard.
    rag_chain.invoke("‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏ã‡∏∑‡πâ‡∏≠‡∏à‡∏±‡∏î‡∏à‡πâ‡∏≤‡∏á‡∏Ç‡∏≠‡∏á ‡∏Å‡∏ü‡∏ú. ‡∏°‡∏µ‡∏Å‡∏µ‡πà‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó")
    rag_chain.invoke("‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏ã‡∏∑‡πâ‡∏≠‡∏à‡∏±‡∏î‡∏à‡πâ‡∏≤‡∏á‡∏Ç‡∏≠‡∏á ‡∏Å‡∏ü‡∏ú. ‡∏°‡∏µ‡∏Å‡∏µ‡πà‡∏ß‡∏¥‡∏ò‡∏µ")
    rag_chain.invoke("‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏ã‡∏∑‡πâ‡∏≠‡∏à‡∏±‡∏î‡∏à‡πâ‡∏≤‡∏á‡∏ß‡∏á‡πÄ‡∏á‡∏¥‡∏ô‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 100,000 ‡∏ö‡∏≤‡∏ó ‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡πÑ‡∏´‡∏ô‡πÑ‡∏î‡πâ‡∏ö‡πâ‡∏≤‡∏á")
    rag_chain.invoke("‡∏Ç‡πâ‡∏≠‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏ô‡∏µ‡πâ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà")
    rag_chain.invoke("‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏ã‡∏∑‡πâ‡∏≠‡∏à‡∏±‡∏î‡∏à‡πâ‡∏≤‡∏á ‡∏°‡∏µ‡∏Å‡∏µ‡πà‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó")
    rag_chain.invoke("‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏ã‡∏∑‡πâ‡∏≠‡∏à‡∏±‡∏î‡∏à‡πâ‡∏≤‡∏á ‡∏°‡∏µ‡∏Å‡∏µ‡πà‡∏ß‡∏¥‡∏ò‡∏µ")
    rag_chain.invoke("‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏ã‡∏∑‡πâ‡∏≠‡∏à‡∏±‡∏î‡∏à‡πâ‡∏≤‡∏á‡∏ß‡∏á‡πÄ‡∏á‡∏¥‡∏ô‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 500,000 ‡∏ö‡∏≤‡∏ó ‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡πÑ‡∏´‡∏ô‡πÑ‡∏î‡πâ‡∏ö‡πâ‡∏≤‡∏á")